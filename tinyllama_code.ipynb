{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohanad-hafez/tinyllama_code_finetuning/blob/main/tinyllama_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKqK9NgkAG9N"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngSUcq9uAG9O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Check so there is a gpu available, a T4(free tier) is enough to run this notebook\n",
        "assert (torch.cuda.is_available()==True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG3wcM7uAG9P",
        "outputId": "3fd268f4-0cf0-4750-ed81-7d7567213246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: axolotl[deepspeed] in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: bitsandbytes==0.45.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.45.0)\n",
            "Requirement already satisfied: triton>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (3.1.0)\n",
            "Requirement already satisfied: liger-kernel==0.4.2 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.4.2)\n",
            "Requirement already satisfied: packaging==23.2 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (23.2)\n",
            "Requirement already satisfied: peft==0.14.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.14.0)\n",
            "Requirement already satisfied: transformers>=4.46.3 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (4.47.1)\n",
            "Requirement already satisfied: tokenizers>=0.20.1 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.21.0)\n",
            "Requirement already satisfied: accelerate==1.2.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.2.0)\n",
            "Requirement already satisfied: datasets==3.1.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (3.1.0)\n",
            "Requirement already satisfied: pydantic==2.6.3 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (2.6.3)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (2.4.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (2.32.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.2.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.19.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.8.0)\n",
            "Requirement already satisfied: optimum==1.16.2 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.16.2)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.1.8)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.4.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.60.0)\n",
            "Requirement already satisfied: numpy<=2.0.1,>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.26.4)\n",
            "Requirement already satisfied: evaluate==0.4.1 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn==1.4.2 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.4.2)\n",
            "Requirement already satisfied: nvidia-ml-py==12.560.30 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (12.560.30)\n",
            "Requirement already satisfied: art in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (6.4)\n",
            "Requirement already satisfied: gradio==3.50.2 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (3.50.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (2.17.1)\n",
            "Requirement already satisfied: python-dotenv==1.0.1 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.0.1)\n",
            "Requirement already satisfied: s3fs>=2024.5.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (2024.9.0)\n",
            "Requirement already satisfied: gcsfs>=2024.5.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (2024.9.0.post1)\n",
            "Requirement already satisfied: trl==0.12.1 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.12.1)\n",
            "Requirement already satisfied: zstandard==0.22.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.22.0)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.7.27)\n",
            "Requirement already satisfied: lm-eval==0.4.4 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.4.4)\n",
            "Requirement already satisfied: langdetect==1.0.9 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.0.9)\n",
            "Requirement already satisfied: immutabledict==4.2.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (4.2.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.13.2 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (4.13.2)\n",
            "Requirement already satisfied: torchao==0.5.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.5.0)\n",
            "Requirement already satisfied: schedulefree==1.3.0 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (1.3)\n",
            "Requirement already satisfied: torch==2.5.1+cu121 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (2.5.1+cu121)\n",
            "Requirement already satisfied: xformers==0.0.28.post3 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.0.28.post3)\n",
            "Requirement already satisfied: deepspeed==0.16.1 in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.16.1)\n",
            "Requirement already satisfied: deepspeed-kernels in /usr/local/lib/python3.10/dist-packages (from axolotl[deepspeed]) (0.0.1.dev1698255861)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==1.2.0->axolotl[deepspeed]) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.2.0->axolotl[deepspeed]) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.2.0->axolotl[deepspeed]) (0.4.5)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.45.0->axolotl[deepspeed]) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0->axolotl[deepspeed]) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0->axolotl[deepspeed]) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0->axolotl[deepspeed]) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0->axolotl[deepspeed]) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0->axolotl[deepspeed]) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0->axolotl[deepspeed]) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0->axolotl[deepspeed]) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.1.0->axolotl[deepspeed]) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.1.0->axolotl[deepspeed]) (3.11.10)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.16.1->axolotl[deepspeed]) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.16.1->axolotl[deepspeed]) (1.1.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.16.1->axolotl[deepspeed]) (1.11.1.3)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.16.1->axolotl[deepspeed]) (9.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.1->axolotl[deepspeed]) (0.18.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (0.115.6)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==0.6.1 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (0.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (3.8.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (3.10.12)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (10.4.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (0.0.20)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (2.10.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (0.34.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.50.2->axolotl[deepspeed]) (11.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect==1.0.9->axolotl[deepspeed]) (1.17.0)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (4.0.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (2.10.2)\n",
            "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (2.13.6)\n",
            "Requirement already satisfied: pytablewriter in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (1.2.1)\n",
            "Requirement already satisfied: rouge-score>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (2.4.3)\n",
            "Requirement already satisfied: sqlitedict in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (2.1.0)\n",
            "Requirement already satisfied: tqdm-multiprocess in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (0.0.11)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (1.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from lm-eval==0.4.4->axolotl[deepspeed]) (10.5.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum==1.16.2->axolotl[deepspeed]) (15.0.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum==1.16.2->axolotl[deepspeed]) (1.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.6.3->axolotl[deepspeed]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.6.3->axolotl[deepspeed]) (2.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2->axolotl[deepspeed]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.4.2->axolotl[deepspeed]) (3.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1+cu121->axolotl[deepspeed]) (3.4.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl==0.12.1->axolotl[deepspeed]) (13.9.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum==1.16.2->axolotl[deepspeed]) (1.3.0)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.5.0->axolotl[deepspeed]) (4.4.2)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.5.0->axolotl[deepspeed]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.5.0->axolotl[deepspeed]) (1.2.1)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from gcsfs>=2024.5.0->axolotl[deepspeed]) (2.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl[deepspeed]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl[deepspeed]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl[deepspeed]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->axolotl[deepspeed]) (2024.12.14)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from s3fs>=2024.5.0->axolotl[deepspeed]) (2.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.3->axolotl[deepspeed]) (2024.11.6)\n",
            "Requirement already satisfied: cmake>=3.24 in /usr/local/lib/python3.10/dist-packages (from deepspeed-kernels->axolotl[deepspeed]) (3.31.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->axolotl[deepspeed]) (2.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->axolotl[deepspeed]) (0.43.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl[deepspeed]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl[deepspeed]) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl[deepspeed]) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl[deepspeed]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl[deepspeed]) (75.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl[deepspeed]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->axolotl[deepspeed]) (3.1.3)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl[deepspeed]) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl[deepspeed]) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl[deepspeed]) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl[deepspeed]) (4.3.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl[deepspeed]) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl[deepspeed]) (1.3.4)\n",
            "Requirement already satisfied: botocore<1.35.89,>=1.35.74 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed]) (1.35.88)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed]) (1.17.0)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed]) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0->axolotl[deepspeed]) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0->axolotl[deepspeed]) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0->axolotl[deepspeed]) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0->axolotl[deepspeed]) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0->axolotl[deepspeed]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0->axolotl[deepspeed]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0->axolotl[deepspeed]) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.1.0->axolotl[deepspeed]) (1.18.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed]) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed]) (1.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->axolotl[deepspeed]) (4.0.11)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl[deepspeed]) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl[deepspeed]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.2->gcsfs>=2024.5.0->axolotl[deepspeed]) (4.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed]) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl[deepspeed]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0->axolotl[deepspeed]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.1.0->axolotl[deepspeed]) (2024.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score>=0.0.4->lm-eval==0.4.4->axolotl[deepspeed]) (3.9.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval==0.4.4->axolotl[deepspeed]) (3.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval==0.4.4->axolotl[deepspeed]) (0.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.5.0->lm-eval==0.4.4->axolotl[deepspeed]) (5.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.50.2->axolotl[deepspeed]) (0.14.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum==1.16.2->axolotl[deepspeed]) (10.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.50.2->axolotl[deepspeed]) (0.41.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib->gcsfs>=2024.5.0->axolotl[deepspeed]) (1.3.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed]) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed]) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed]) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed]) (1.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->axolotl[deepspeed]) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.50.2->axolotl[deepspeed]) (1.0.7)\n",
            "Requirement already satisfied: DataProperty<2,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval==0.4.4->axolotl[deepspeed]) (1.1.0)\n",
            "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval==0.4.4->axolotl[deepspeed]) (1.1.3)\n",
            "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval==0.4.4->axolotl[deepspeed]) (3.2.2)\n",
            "Requirement already satisfied: tabledata<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval==0.4.4->axolotl[deepspeed]) (1.3.4)\n",
            "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from pytablewriter->lm-eval==0.4.4->axolotl[deepspeed]) (0.1.7)\n",
            "Requirement already satisfied: typepy<2,>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval==0.4.4->axolotl[deepspeed]) (1.3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.12.1->axolotl[deepspeed]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.12.1->axolotl[deepspeed]) (2.18.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.89,>=1.35.74->aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.5.0->axolotl[deepspeed]) (1.0.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->axolotl[deepspeed]) (5.0.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed]) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs>=2024.5.0->axolotl[deepspeed]) (1.25.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl[deepspeed]) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1->axolotl[deepspeed]) (0.1.2)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval==0.4.4->axolotl[deepspeed]) (5.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs>=2024.5.0->axolotl[deepspeed]) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs>=2024.5.0->axolotl[deepspeed]) (3.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.50.2->axolotl[deepspeed]) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.50.2->axolotl[deepspeed]) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-build-isolation axolotl[deepspeed]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYc-xzwEAG9P"
      },
      "source": [
        "## Hugging Face login (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "b70153ca8d034b31bbdd009707aa97c4",
            "f0084a9344c64327922190e1f3041d96"
          ]
        },
        "id": "uHdqJsmrAG9P",
        "outputId": "becd1ffb-1fd2-42fa-e798-8eece9fdc9a7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b70153ca8d034b31bbdd009707aa97c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "WeXG7671C4o8",
        "outputId": "05f78b45-fd90-4deb-b11c-1e95ed583cbf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 37\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 37\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KthNUIVZAG9Q"
      },
      "source": [
        "## Example configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymDBg46TAG9Q"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "yaml_string = \"\"\"\n",
        "base_model: TinyLlama/TinyLlama_v1.1\n",
        "\n",
        "model_type: LlamaForCausalLM\n",
        "tokenizer_type: LlamaTokenizer\n",
        "\n",
        "hub_model_id: EvolCodeTinyLlama\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "    - path: mlabonne/Evol-Instruct-Python-1k\n",
        "      type: alpaca\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.02\n",
        "output_dir: ./qlora-out\n",
        "\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "\n",
        "sequence_len: 2048\n",
        "sample_packing: true\n",
        "eval_sample_packing: False\n",
        "\n",
        "lora_r: 32\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: axolotl\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 2\n",
        "micro_batch_size: 2\n",
        "num_epochs: 3\n",
        "optimizer: paged_adamw_32bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.0002\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: false\n",
        "\n",
        "warmup_steps: 10\n",
        "eval_steps: 0.01\n",
        "save_strategy: epoch\n",
        "save_steps:\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Convert the YAML string to a Python dictionary\n",
        "yaml_dict = yaml.safe_load(yaml_string)\n",
        "\n",
        "# Specify your file path\n",
        "file_path = 'tinyllama.yaml'\n",
        "\n",
        "# Write the YAML file\n",
        "with open(file_path, 'w') as file:\n",
        "    yaml.dump(yaml_dict, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xU4rf9KAG9Q"
      },
      "source": [
        "Above we have a configuration file with base LLM model and datasets specified, among many other things. Axolotl can automatically detect whether the specified datasets are on HuggingFace repo or local machine.\n",
        "\n",
        "The Axolotl configuration options encompass model and dataset selection, data pre-processing, and training. Let's go through them line by line:\n",
        "\n",
        "*   \"base model\": String value, specifies the underlying pre-trained LLM that will be used for finetuning\n",
        "\n",
        "Next we have options for model weights quantization. Quantization allows for reduction in occupied memory on GPUs.\n",
        "\n",
        "*   \"load_in_8bit\": Boolean value, whether to quantize the model weights into 8-bit integer.\n",
        "\n",
        "*   \"load_in_4bit\": Boolean value, whether to quantize the model weights into 4-bit integer.\n",
        "\n",
        "*   \"strict\": Boolean value. If false, it allows for overriding established configuration options in the yaml file when executing in command-line interface.\n",
        "\n",
        "*   \"datasets\": a list of dicts that contain path and type of data sets as well as other optional configurations where datasets are concerned. Supports multiple datasets.\n",
        "\n",
        "*   \"val_set_size\": Either a float value less than one or an integer less than the total size of dataset. Sets the size of validation set from the whole dataset. If float, sets the proportion of the dataset assigned for validation. If integer, sets the direct size of validation set.\n",
        "\n",
        "*   \"output_dir\": String value. Path of trained model.\n",
        "\n",
        "For data preprocessing:\n",
        "\n",
        "*   \"sequence_len\": Integer. Specifies the maximum sequence length of the input. Typically 2048 or less.\n",
        "\n",
        "*   \"pad_to_sequence_len\": Boolean. Padding input to maximum sequence length.\n",
        "\n",
        "*   \"sample_packing\": Boolean. Specifies whether to use multi-packing with block diagonal attention.\n",
        "\n",
        "*   \"special_tokens\": Python dict, optional. Allows users to specify the additional special tokens to be ignored by the tokenizer.\n",
        "\n",
        "For LoRA configuration and its hyperparamters:\n",
        "\n",
        "*   \"adapter\": String. Either \"lora\" or \"qlora\", depending on user's choice.\n",
        "\n",
        "*   \"lora_model_dir\": String, Optional. Path to directory that contains LoRA model, if there is already a trained LoRA model the user would like to use.\n",
        "\n",
        "*   \"lora_r\": Integer. Refers to the rank of LoRA decomposition matrices. Higher value will reduce LoRA efficiency. Recommended to be set to 8.\n",
        "\n",
        "*   \"lora_alpha\": Integer. Scale the weight matrices by $\\frac{\\text{lora_alpha}}{\\text{lora_r}}$Recommended to be fixed at 16.\n",
        "\n",
        "*   \"lora_dropout\": Float that is 1 or less. The dropout probability of a lora layer.\n",
        "\n",
        "*   \"lora_target_linear\": Boolean. If true, lora will target all linear modules in the transformers architecture.\n",
        "\n",
        "*   \"lora_modules_to_save\": If you added new tokens to the tokenizer, you may need to save some LoRA modules because they need to know the new tokens.\n",
        "\n",
        "See [LoRA](https://arxiv.org/abs/2106.09685) for detailed explanation of LoRA implementation.\n",
        "\n",
        "For the training configurations:\n",
        "\n",
        "*   \"gradient_accumulation_steps\": Integer. The number of steps over which to accumulate gradient for batch training. E.g. if 2, backprop is performed every two steps.\n",
        "\n",
        "*   \"micro_batch_size\": Integer. Batch size per gpu / gradient_accumulation_steps\n",
        "\n",
        "*   \"num_epochs\": Integer. Number of epochs. One epoch is when training has looped over every batch in the whole data set once.\n",
        "\n",
        "*   \"optimizer\": The optimizer to use for the training.\n",
        "\n",
        "*   \"learning_rate\": The learning rate.\n",
        "\n",
        "*   \"lr_scheduler\": The learning rate scheduler to use for adjusting learning rate during training.\n",
        "\n",
        "*   \"train_on_inputs\": Boolean. Whether to ignore or include the user's prompt from the training labels.\n",
        "\n",
        "*   \"group_by_length\": Boolean. Whether to group similarly sized data to minimize padding.\n",
        "\n",
        "*   \"bf16\": Either \"auto\", \"true\", or \"false\". Whether to use CUDA bf16 floating point format. If set to \"auto\", will automatically apply bf16 should the gpu supports it.\n",
        "\n",
        "*   \"fp16\": Optional. Specifies whether to use CUDA fp16. Automatically set to true if \"bf16\" is set to true. Otherwise false.\n",
        "\n",
        "*   \"tf32\": Boolean. Whether to use CUDA tf32. Will override bf16.\n",
        "\n",
        "*   \"gradient_checkpointing\": Boolean. Whether to use gradient checkpointing https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing\n",
        "\n",
        "*   \"gradient_checkpointing_kwargs\": Python Dict. Fed into the trainer.\n",
        "\n",
        "*   \"logging_steps\": Integer. Log training information over every specified number of steps.\n",
        "\n",
        "*   \"flash_attention\": Boolean. Whether to use the [flash attention](https://github.com/Dao-AILab/flash-attention) mechanism.\n",
        "\n",
        "*   \"sdp_attention\": Boolean. Whether to use the Scaled Dot Product attention mechanism (the attention mechanism in the [original implementation](https://arxiv.org/abs/1706.03762) of transformers.)\n",
        "\n",
        "*   \"warmup_steps\": Integer. The number of pre-training steps where a very low learning rate is used.\n",
        "\n",
        "*   \"evals_per_epoch\": Integer. Number of evaluations to be performed within one training epoch.\n",
        "\n",
        "*   \"saves_per_epoch\": Integer. Number of times the model is saved in one training epoch.\n",
        "\n",
        "*   \"weight_decay\": Positive Float. Sets the \"strength\" of weight decay (i.e. setting the coefficient of L2 regularization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtvVz1aeAG9R"
      },
      "source": [
        "The above is but a snippet aiming to get users familiarized with the types of streamlined configuration options axolotl provides. For a full list of configuration options, see [here](https://axolotl-ai-cloud.github.io/axolotl/docs/config.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoZl_IBNAG9R"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "X22F5qNxAG9R",
        "outputId": "02b130b6-f428-4a7a-938c-5518f4dd6919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-01-02 10:21:40.512060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-02 10:21:40.548991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-02 10:21:40.559974: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-02 10:21:40.586321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-02 10:21:42.318750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2025-01-02 10:21:45,654] [INFO] [datasets.<module>:54] [PID:6219] PyTorch version 2.5.1+cu121 available.\n",
            "[2025-01-02 10:21:45,656] [INFO] [datasets.<module>:66] [PID:6219] Polars version 1.9.0 available.\n",
            "[2025-01-02 10:21:45,657] [INFO] [datasets.<module>:77] [PID:6219] Duckdb version 1.1.3 available.\n",
            "[2025-01-02 10:21:45,658] [INFO] [datasets.<module>:112] [PID:6219] TensorFlow version 2.17.1 available.\n",
            "[2025-01-02 10:21:45,659] [INFO] [datasets.<module>:125] [PID:6219] JAX version 0.4.33 available.\n",
            "[2025-01-02 10:21:48,233] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "df: /root/.triton/autotune: No such file or directory\n",
            "[2025-01-02 10:21:48,330] [INFO] [root.spawn:60] [PID:6219] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpk9pzc_n_/test.c -o /tmp/tmpk9pzc_n_/test.o\n",
            "[2025-01-02 10:21:48,358] [INFO] [root.spawn:60] [PID:6219] x86_64-linux-gnu-gcc /tmp/tmpk9pzc_n_/test.o -laio -o /tmp/tmpk9pzc_n_/a.out\n",
            "[2025-01-02 10:21:49,052] [INFO] [root.spawn:60] [PID:6219] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpljo1a2p8/test.c -o /tmp/tmpljo1a2p8/test.o\n",
            "[2025-01-02 10:21:49,070] [INFO] [root.spawn:60] [PID:6219] x86_64-linux-gnu-gcc /tmp/tmpljo1a2p8/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmpljo1a2p8/a.out\n",
            "[2025-01-02 10:21:49,124] [INFO] [root.spawn:60] [PID:6219] x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -c /tmp/tmpxvmsq_12/test.c -o /tmp/tmpxvmsq_12/test.o\n",
            "[2025-01-02 10:21:49,144] [INFO] [root.spawn:60] [PID:6219] x86_64-linux-gnu-gcc /tmp/tmpxvmsq_12/test.o -laio -o /tmp/tmpxvmsq_12/a.out\n",
            "/usr/local/lib/python3.10/dist-packages/axolotl/monkeypatch/relora.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
            "  from torch.distributed.optim import ZeroRedundancyOptimizer\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_kwargs\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n",
            "[2025-01-02 10:21:52,836] [INFO] [axolotl.utils.config.models.input.check_eval_packing:1136] [PID:6219] [RANK:0] setting `remove_unused_columns: false` for when sample_packing and eval_sample_packing don't match\u001b[39m\n",
            "\u001b[33m[2025-01-02 10:21:52,836] [WARNING] [axolotl.utils.config.models.input.hint_sample_packing_padding:937] [PID:6219] [RANK:0] `pad_to_sequence_len: true` is recommended when using sample_packing\u001b[39m\n",
            "\u001b[33m[2025-01-02 10:21:52,836] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_wo_flash:920] [PID:6219] [RANK:0] sample_packing without flash_attention or sdp_attention does not handle cross-attention.\u001b[39m\n",
            "config.json: 100% 560/560 [00:00<00:00, 3.61MB/s]\n",
            "[2025-01-02 10:21:53,445] [INFO] [axolotl.normalize_config:211] [PID:6219] [RANK:0] cuda memory usage baseline: 0.000GB (+0.002GB cache, +0.352GB misc)\u001b[39m\n",
            "\n",
            "     #@@ #@@      @@# @@#\n",
            "    @@  @@          @@  @@           =@@#                               @@                 #@    =@@#.\n",
            "    @@    #@@@@@@@@@    @@           #@#@=                              @@                 #@     .=@@\n",
            "      #@@@@@@@@@@@@@@@@@            =@# @#     ##=     ##    =####=+    @@      =#####+  =#@@###.   @@\n",
            "    @@@@@@@@@@/  +@@/  +@@          #@  =@=     #@=   @@   =@#+  +#@#   @@    =@#+  +#@#   #@.      @@\n",
            "    @@@@@@@@@@  ##@@  ##@@         =@#   @#      =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
            "     @@@@@@@@@@@@@@@@@@@@          #@=+++#@=      =@@#     @@      @@   @@    @@      #@   #@       @@\n",
            "                                  =@#=====@@     =@# @#    @@      @@   @@    @@      #@   #@       @@\n",
            "    @@@@@@@@@@@@@@@@  @@@@        #@      #@=   #@=  +@@   #@#    =@#   @@.   =@#    =@#   #@.      @@\n",
            "                                 =@#       @#  #@=     #@   =#@@@@#=    +#@@=  +#@@@@#=    .##@@+   @@\n",
            "    @@@@  @@@@@@@@@@@@@@@@\n",
            "\n",
            "tokenizer_config.json: 100% 776/776 [00:00<00:00, 4.73MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 61.3MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 2.19MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:01<00:00, 1.60MB/s]\n",
            "[2025-01-02 10:21:58,494] [DEBUG] [axolotl.load_tokenizer:296] [PID:6219] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2025-01-02 10:21:58,494] [DEBUG] [axolotl.load_tokenizer:297] [PID:6219] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2025-01-02 10:21:58,494] [DEBUG] [axolotl.load_tokenizer:298] [PID:6219] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2025-01-02 10:21:58,494] [DEBUG] [axolotl.load_tokenizer:299] [PID:6219] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2025-01-02 10:21:58,494] [INFO] [axolotl.load_tokenizer:313] [PID:6219] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2025-01-02 10:21:58,494] [INFO] [axolotl.load_tokenized_prepared_datasets:216] [PID:6219] [RANK:0] Unable to find prepared dataset in last_run_prepared/d4980cf45b7d886e0a2393e374eb74e7\u001b[39m\n",
            "[2025-01-02 10:21:58,494] [INFO] [axolotl.load_tokenized_prepared_datasets:217] [PID:6219] [RANK:0] Loading raw datasets...\u001b[39m\n",
            "\u001b[33m[2025-01-02 10:21:58,495] [WARNING] [axolotl.load_tokenized_prepared_datasets:219] [PID:6219] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
            "[2025-01-02 10:21:58,495] [INFO] [axolotl.load_tokenized_prepared_datasets:226] [PID:6219] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
            "README.md: 100% 756/756 [00:00<00:00, 4.99MB/s]\n",
            "(…)-00000-of-00001-dce3832cbf0b04e0.parquet: 100% 2.32M/2.32M [00:00<00:00, 9.59MB/s]\n",
            "Generating train split: 100% 1000/1000 [00:00<00:00, 15572.64 examples/s]\n",
            "[2025-01-02 10:22:06,272] [INFO] [axolotl.get_dataset_wrapper:613] [PID:6219] [RANK:0] Loading dataset with base_type: alpaca and prompt_style: None\u001b[39m\n",
            "Tokenizing Prompts (num_proc=2): 100% 1000/1000 [00:12<00:00, 80.34 examples/s]\n",
            "[2025-01-02 10:22:18,823] [DEBUG] [axolotl.process_datasets_for_packing:189] [PID:6219] [RANK:0] min_input_len: 1277\u001b[39m\n",
            "[2025-01-02 10:22:18,824] [DEBUG] [axolotl.process_datasets_for_packing:191] [PID:6219] [RANK:0] max_input_len: 2083\u001b[39m\n",
            "Dropping Long Sequences (num_proc=2): 100% 1000/1000 [00:02<00:00, 456.07 examples/s]\n",
            "\u001b[33m[2025-01-02 10:22:21,102] [WARNING] [axolotl.process_datasets_for_packing:215] [PID:6219] [RANK:0] Dropped 13 long samples from train dataset\u001b[39m\n",
            "Drop Samples with Zero Trainable Tokens (num_proc=2): 100% 987/987 [00:02<00:00, 442.60 examples/s]\n",
            "Add position_id column (Sample Packing) (num_proc=2): 100% 987/987 [00:01<00:00, 798.73 examples/s] \n",
            "[2025-01-02 10:22:24,760] [INFO] [axolotl.load_tokenized_prepared_datasets:486] [PID:6219] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/d4980cf45b7d886e0a2393e374eb74e7\u001b[39m\n",
            "Saving the dataset (1/1 shards): 100% 987/987 [00:00<00:00, 13080.24 examples/s]\n",
            "[2025-01-02 10:22:24,856] [DEBUG] [axolotl.calculate_total_num_steps:342] [PID:6219] [RANK:0] total_num_tokens: 1_475_271\u001b[39m\n",
            "[2025-01-02 10:22:24,877] [DEBUG] [axolotl.calculate_total_num_steps:360] [PID:6219] [RANK:0] `total_supervised_tokens: 1_042_722`\u001b[39m\n",
            "[2025-01-02 10:22:33,239] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:6219] [RANK:0] gather_len_batches: [484]\u001b[39m\n",
            "[2025-01-02 10:22:33,239] [DEBUG] [axolotl.calculate_total_num_steps:412] [PID:6219] [RANK:0] data_loader_len: 241\u001b[39m\n",
            "[2025-01-02 10:22:33,240] [INFO] [axolotl.calc_sample_packing_eff_est:418] [PID:6219] [RANK:0] sample_packing_eff_est across ranks: [0.7449298531217684]\u001b[39m\n",
            "[2025-01-02 10:22:33,240] [DEBUG] [axolotl.calculate_total_num_steps:430] [PID:6219] [RANK:0] sample_packing_eff_est: 0.75\u001b[39m\n",
            "[2025-01-02 10:22:33,240] [DEBUG] [axolotl.calculate_total_num_steps:438] [PID:6219] [RANK:0] total_num_steps: 723\u001b[39m\n",
            "[2025-01-02 10:22:33,248] [DEBUG] [axolotl.train.train:66] [PID:6219] [RANK:0] loading tokenizer... TinyLlama/TinyLlama_v1.1\u001b[39m\n",
            "[2025-01-02 10:22:34,347] [DEBUG] [axolotl.load_tokenizer:296] [PID:6219] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
            "[2025-01-02 10:22:34,347] [DEBUG] [axolotl.load_tokenizer:297] [PID:6219] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
            "[2025-01-02 10:22:34,347] [DEBUG] [axolotl.load_tokenizer:298] [PID:6219] [RANK:0] PAD: 2 / </s>\u001b[39m\n",
            "[2025-01-02 10:22:34,347] [DEBUG] [axolotl.load_tokenizer:299] [PID:6219] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
            "[2025-01-02 10:22:34,347] [INFO] [axolotl.load_tokenizer:313] [PID:6219] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
            "[2025-01-02 10:22:34,347] [DEBUG] [axolotl.train.train:98] [PID:6219] [RANK:0] loading model and peft_config...\u001b[39m\n",
            "[2025-01-02 10:22:34,599] [INFO] [axolotl.monkeypatch.trainer_grad_accum.patch_forward_for_ga:203] [PID:6219] [RANK:0] patching forward\u001b[39m\n",
            "[2025-01-02 10:22:34,602] [INFO] [axolotl.patch_llama_derived_model:560] [PID:6219] [RANK:0] patching llama _prepare_4d_causal_attention_mask*\u001b[39m\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "pytorch_model.bin: 100% 4.40G/4.40G [01:44<00:00, 41.9MB/s]\n",
            "generation_config.json: 100% 129/129 [00:00<00:00, 602kB/s]\n",
            "[2025-01-02 10:24:37,161] [INFO] [axolotl.load_model:1088] [PID:6219] [RANK:0] cuda memory usage after model load: 0.719GB (+0.043GB cache, +0.368GB misc)\u001b[39m\n",
            "[2025-01-02 10:24:37,201] [INFO] [axolotl.prepare_model:1008] [PID:6219] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
            "[2025-01-02 10:24:37,205] [INFO] [axolotl.load_model:1121] [PID:6219] [RANK:0] Converting modules to torch.bfloat16\u001b[39m\n",
            "[2025-01-02 10:24:37,211] [INFO] [axolotl.load_lora:1310] [PID:6219] [RANK:0] found linear modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[39m\n",
            "trainable params: 25,231,360 || all params: 1,125,279,744 || trainable%: 2.2422\n",
            "[2025-01-02 10:24:37,770] [INFO] [axolotl.load_model:1182] [PID:6219] [RANK:0] cuda memory usage after adapters: 0.813GB (+0.535GB cache, +0.368GB misc)\u001b[39m\n",
            "/usr/local/lib/python3.10/dist-packages/axolotl/core/trainer_builder.py:444: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AxolotlTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*_args, **kwargs)\n",
            "[2025-01-02 10:24:39,450] [INFO] [axolotl.train.train:141] [PID:6219] [RANK:0] Pre-saving adapter config to ./qlora-out\u001b[39m\n",
            "[2025-01-02 10:24:39,454] [INFO] [axolotl.train.train:178] [PID:6219] [RANK:0] Starting trainer...\u001b[39m\n",
            "[2025-01-02 10:24:39,910] [INFO] [axolotl.utils.samplers.multipack.calc_min_len:197] [PID:6219] [RANK:0] gather_len_batches: [484]\u001b[39m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohanad\u001b[0m (\u001b[33mmohanad-king-saud-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
            "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
            "  return self.__pydantic_serializer__.to_python(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250102_102440-1jn5q5qa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstoic-gorge-5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mohanad-king-saud-university/axolotl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mohanad-king-saud-university/axolotl/runs/1jn5q5qa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "[2025-01-02 10:24:42,072] [INFO] [axolotl.callbacks.on_train_begin:814] [PID:6219] [RANK:0] The Axolotl config has been saved to the WandB run under files.\u001b[39m\n",
            "{'loss': 0.7502, 'grad_norm': 0.1458759903907776, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
            "  0% 1/723 [00:30<6:01:20, 30.03s/it][2025-01-02 10:25:37,062] [INFO] [axolotl.callbacks.on_step_end:130] [PID:6219] [RANK:0] cuda memory usage while training: 0.928GB (+7.949GB cache, +0.636GB misc)\u001b[39m\n",
            "{'loss': 0.7483, 'grad_norm': 0.16933971643447876, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}\n",
            "{'loss': 0.5608, 'grad_norm': 0.13856862485408783, 'learning_rate': 6e-06, 'epoch': 0.01}\n",
            "{'loss': 0.4642, 'grad_norm': 0.11054331064224243, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.02}\n",
            "{'loss': 0.5065, 'grad_norm': 0.16979484260082245, 'learning_rate': 1e-05, 'epoch': 0.02}\n",
            "{'loss': 0.543, 'grad_norm': 0.1266026794910431, 'learning_rate': 1.2e-05, 'epoch': 0.02}\n",
            "{'loss': 0.6644, 'grad_norm': 0.17686600983142853, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.03}\n",
            "{'loss': 0.6745, 'grad_norm': 0.12942315638065338, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.03}\n",
            "  1% 8/723 [03:45<5:40:39, 28.59s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 10:28:57,689] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7438298463821411, 'eval_runtime': 31.6893, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.316, 'epoch': 0.03}\n",
            "  1% 8/723 [04:16<5:40:39, 28.59s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5767, 'grad_norm': 0.1428147405385971, 'learning_rate': 1.8e-05, 'epoch': 0.04}\n",
            "{'loss': 0.7152, 'grad_norm': 0.13915276527404785, 'learning_rate': 2e-05, 'epoch': 0.04}\n",
            "{'loss': 0.7842, 'grad_norm': 0.1572343111038208, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.05}\n",
            "{'loss': 0.4902, 'grad_norm': 0.10569049417972565, 'learning_rate': 2.4e-05, 'epoch': 0.05}\n",
            "{'loss': 0.6126, 'grad_norm': 0.1311621218919754, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.05}\n",
            "{'loss': 0.5474, 'grad_norm': 0.1286897212266922, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.06}\n",
            "{'loss': 0.8256, 'grad_norm': 0.16260085999965668, 'learning_rate': 3e-05, 'epoch': 0.06}\n",
            "{'loss': 0.698, 'grad_norm': 0.12520423531532288, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.07}\n",
            "  2% 16/723 [07:58<5:33:28, 28.30s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.36s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 10:33:11,298] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.739208996295929, 'eval_runtime': 31.7097, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.315, 'epoch': 0.07}\n",
            "  2% 16/723 [08:30<5:33:28, 28.30s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.7103, 'grad_norm': 0.15560485422611237, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.07}\n",
            "{'loss': 0.4471, 'grad_norm': 0.11390089988708496, 'learning_rate': 3.6e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5869, 'grad_norm': 0.13174638152122498, 'learning_rate': 3.8e-05, 'epoch': 0.08}\n",
            "{'loss': 0.9229, 'grad_norm': 0.15204927325248718, 'learning_rate': 4e-05, 'epoch': 0.08}\n",
            "{'loss': 0.6691, 'grad_norm': 0.14714735746383667, 'learning_rate': 4.2e-05, 'epoch': 0.09}\n",
            "{'loss': 0.6031, 'grad_norm': 0.1420106440782547, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5525, 'grad_norm': 0.12911415100097656, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.1}\n",
            "{'loss': 0.5908, 'grad_norm': 0.13183890283107758, 'learning_rate': 4.8e-05, 'epoch': 0.1}\n",
            "  3% 24/723 [11:51<5:13:35, 26.92s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 10:37:04,142] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.726381778717041, 'eval_runtime': 31.6742, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.316, 'epoch': 0.1}\n",
            "  3% 24/723 [12:23<5:13:35, 26.92s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.651, 'grad_norm': 0.1488937884569168, 'learning_rate': 5e-05, 'epoch': 0.1}\n",
            "{'loss': 0.4429, 'grad_norm': 0.1196308508515358, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.11}\n",
            "{'loss': 0.6181, 'grad_norm': 0.1534818410873413, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.11}\n",
            "{'loss': 0.8748, 'grad_norm': 0.17406824231147766, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5773, 'grad_norm': 0.15293346345424652, 'learning_rate': 5.8e-05, 'epoch': 0.12}\n",
            "{'loss': 0.8367, 'grad_norm': 0.1596481055021286, 'learning_rate': 6e-05, 'epoch': 0.12}\n",
            "{'loss': 0.8009, 'grad_norm': 0.18552887439727783, 'learning_rate': 6.2e-05, 'epoch': 0.13}\n",
            "{'loss': 0.4434, 'grad_norm': 0.15516717731952667, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.13}\n",
            "  4% 32/723 [15:46<4:51:33, 25.32s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.94s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 10:40:58,870] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.709286093711853, 'eval_runtime': 31.7326, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 0.13}\n",
            "  4% 32/723 [16:18<4:51:33, 25.32s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5631, 'grad_norm': 0.1559860110282898, 'learning_rate': 6.6e-05, 'epoch': 0.14}\n",
            "{'loss': 0.545, 'grad_norm': 0.1596999168395996, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.14}\n",
            "{'loss': 0.64, 'grad_norm': 0.17479273676872253, 'learning_rate': 7e-05, 'epoch': 0.14}\n",
            "{'loss': 0.6494, 'grad_norm': 0.17331990599632263, 'learning_rate': 7.2e-05, 'epoch': 0.15}\n",
            "{'loss': 0.6247, 'grad_norm': 0.15172410011291504, 'learning_rate': 7.4e-05, 'epoch': 0.15}\n",
            "{'loss': 0.6478, 'grad_norm': 0.22366616129875183, 'learning_rate': 7.6e-05, 'epoch': 0.16}\n",
            "{'loss': 0.6133, 'grad_norm': 0.178421750664711, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.16}\n",
            "{'loss': 0.4813, 'grad_norm': 0.17173852026462555, 'learning_rate': 8e-05, 'epoch': 0.17}\n",
            "  6% 40/723 [19:53<5:06:01, 26.88s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.37s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.71s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.87s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.94s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.99s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 10:45:05,964] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6915823221206665, 'eval_runtime': 31.7644, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 0.17}\n",
            "  6% 40/723 [20:25<5:06:01, 26.88s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.734, 'grad_norm': 0.20507077872753143, 'learning_rate': 8.2e-05, 'epoch': 0.17}\n",
            "{'loss': 0.756, 'grad_norm': 0.20073330402374268, 'learning_rate': 8.4e-05, 'epoch': 0.17}\n",
            "{'loss': 0.4889, 'grad_norm': 0.1674206554889679, 'learning_rate': 8.6e-05, 'epoch': 0.18}\n",
            "{'loss': 0.5547, 'grad_norm': 0.198304682970047, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.18}\n",
            "{'loss': 0.6729, 'grad_norm': 0.20817802846431732, 'learning_rate': 9e-05, 'epoch': 0.19}\n",
            "{'loss': 0.4916, 'grad_norm': 0.19078457355499268, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.19}\n",
            "{'loss': 0.556, 'grad_norm': 0.1864233762025833, 'learning_rate': 9.4e-05, 'epoch': 0.19}\n",
            "{'loss': 0.4302, 'grad_norm': 0.2089635729789734, 'learning_rate': 9.6e-05, 'epoch': 0.2}\n",
            "  7% 48/723 [23:52<5:07:49, 27.36s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.36s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.87s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 10:49:04,832] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6777549982070923, 'eval_runtime': 31.7159, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.315, 'epoch': 0.2}\n",
            "  7% 48/723 [24:23<5:07:49, 27.36s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5459, 'grad_norm': 0.1947522610425949, 'learning_rate': 9.8e-05, 'epoch': 0.2}\n",
            "{'loss': 0.4873, 'grad_norm': 0.19243614375591278, 'learning_rate': 0.0001, 'epoch': 0.21}\n",
            "{'loss': 0.5108, 'grad_norm': 0.1588056981563568, 'learning_rate': 0.00010200000000000001, 'epoch': 0.21}\n",
            "{'loss': 0.564, 'grad_norm': 0.15984226763248444, 'learning_rate': 0.00010400000000000001, 'epoch': 0.22}\n",
            "{'loss': 0.5371, 'grad_norm': 0.18339844048023224, 'learning_rate': 0.00010600000000000002, 'epoch': 0.22}\n",
            "{'loss': 0.866, 'grad_norm': 0.18503622710704803, 'learning_rate': 0.00010800000000000001, 'epoch': 0.22}\n",
            "{'loss': 0.558, 'grad_norm': 0.18359896540641785, 'learning_rate': 0.00011000000000000002, 'epoch': 0.23}\n",
            "{'loss': 0.5735, 'grad_norm': 0.19931918382644653, 'learning_rate': 0.00011200000000000001, 'epoch': 0.23}\n",
            "  8% 56/723 [27:53<4:55:27, 26.58s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.87s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.94s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.99s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 10:53:05,677] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6693571209907532, 'eval_runtime': 31.7482, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 0.23}\n",
            "  8% 56/723 [28:24<4:55:27, 26.58s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.4796, 'grad_norm': 0.164516881108284, 'learning_rate': 0.00011399999999999999, 'epoch': 0.24}\n",
            "{'loss': 0.4967, 'grad_norm': 0.19000646471977234, 'learning_rate': 0.000116, 'epoch': 0.24}\n",
            "{'loss': 0.6961, 'grad_norm': 0.23151065409183502, 'learning_rate': 0.000118, 'epoch': 0.24}\n",
            "{'loss': 0.707, 'grad_norm': 0.1826700121164322, 'learning_rate': 0.00012, 'epoch': 0.25}\n",
            "{'loss': 0.5164, 'grad_norm': 0.16722741723060608, 'learning_rate': 0.000122, 'epoch': 0.25}\n",
            "{'loss': 0.6513, 'grad_norm': 0.17889156937599182, 'learning_rate': 0.000124, 'epoch': 0.26}\n",
            "{'loss': 0.6855, 'grad_norm': 0.17856258153915405, 'learning_rate': 0.000126, 'epoch': 0.26}\n",
            "{'loss': 0.6616, 'grad_norm': 0.18541236221790314, 'learning_rate': 0.00012800000000000002, 'epoch': 0.27}\n",
            "  9% 64/723 [32:13<5:12:07, 28.42s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 10:57:26,345] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6630915403366089, 'eval_runtime': 31.6953, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.316, 'epoch': 0.27}\n",
            "  9% 64/723 [32:45<5:12:07, 28.42s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5819, 'grad_norm': 0.16668853163719177, 'learning_rate': 0.00013000000000000002, 'epoch': 0.27}\n",
            "{'loss': 0.525, 'grad_norm': 0.1869840919971466, 'learning_rate': 0.000132, 'epoch': 0.27}\n",
            "{'loss': 0.4785, 'grad_norm': 0.18993836641311646, 'learning_rate': 0.000134, 'epoch': 0.28}\n",
            "{'loss': 0.2962, 'grad_norm': 0.14265719056129456, 'learning_rate': 0.00013600000000000003, 'epoch': 0.28}\n",
            "{'loss': 0.6749, 'grad_norm': 0.20105375349521637, 'learning_rate': 0.000138, 'epoch': 0.29}\n",
            "{'loss': 0.4958, 'grad_norm': 0.1794358789920807, 'learning_rate': 0.00014, 'epoch': 0.29}\n",
            "{'loss': 0.6795, 'grad_norm': 0.20828209817409515, 'learning_rate': 0.000142, 'epoch': 0.29}\n",
            "{'loss': 0.399, 'grad_norm': 0.16833005845546722, 'learning_rate': 0.000144, 'epoch': 0.3}\n",
            " 10% 72/723 [36:10<4:33:35, 25.22s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:01:22,736] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6594568490982056, 'eval_runtime': 31.6582, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 0.3}\n",
            " 10% 72/723 [36:41<4:33:35, 25.22s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.6434, 'grad_norm': 0.21430358290672302, 'learning_rate': 0.000146, 'epoch': 0.3}\n",
            "{'loss': 0.4238, 'grad_norm': 0.181999072432518, 'learning_rate': 0.000148, 'epoch': 0.31}\n",
            "{'loss': 0.5625, 'grad_norm': 0.1779814511537552, 'learning_rate': 0.00015000000000000001, 'epoch': 0.31}\n",
            "{'loss': 0.6129, 'grad_norm': 0.21842524409294128, 'learning_rate': 0.000152, 'epoch': 0.31}\n",
            "{'loss': 0.7003, 'grad_norm': 0.22045806050300598, 'learning_rate': 0.000154, 'epoch': 0.32}\n",
            "{'loss': 0.4056, 'grad_norm': 0.164620503783226, 'learning_rate': 0.00015600000000000002, 'epoch': 0.32}\n",
            "{'loss': 0.5298, 'grad_norm': 0.24765117466449738, 'learning_rate': 0.00015800000000000002, 'epoch': 0.33}\n",
            "{'loss': 0.4785, 'grad_norm': 0.1712838113307953, 'learning_rate': 0.00016, 'epoch': 0.33}\n",
            " 11% 80/723 [40:07<4:47:26, 26.82s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:05:20,488] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6548817753791809, 'eval_runtime': 31.6649, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 0.33}\n",
            " 11% 80/723 [40:39<4:47:26, 26.82s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.507, 'grad_norm': 0.19644494354724884, 'learning_rate': 0.000162, 'epoch': 0.34}\n",
            "{'loss': 0.8149, 'grad_norm': 0.19678780436515808, 'learning_rate': 0.000164, 'epoch': 0.34}\n",
            "{'loss': 0.5289, 'grad_norm': 0.15445782244205475, 'learning_rate': 0.000166, 'epoch': 0.34}\n",
            "{'loss': 0.5019, 'grad_norm': 0.19972001016139984, 'learning_rate': 0.000168, 'epoch': 0.35}\n",
            "{'loss': 0.5971, 'grad_norm': 0.22589825093746185, 'learning_rate': 0.00017, 'epoch': 0.35}\n",
            "{'loss': 0.6712, 'grad_norm': 0.2210131585597992, 'learning_rate': 0.000172, 'epoch': 0.36}\n",
            "{'loss': 0.5513, 'grad_norm': 0.2168228179216385, 'learning_rate': 0.000174, 'epoch': 0.36}\n",
            "{'loss': 0.4842, 'grad_norm': 0.16859130561351776, 'learning_rate': 0.00017600000000000002, 'epoch': 0.36}\n",
            " 12% 88/723 [44:11<4:55:10, 27.89s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.36s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:09:23,597] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6498054265975952, 'eval_runtime': 31.6889, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.316, 'epoch': 0.36}\n",
            " 12% 88/723 [44:42<4:55:10, 27.89s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5887, 'grad_norm': 0.20936211943626404, 'learning_rate': 0.00017800000000000002, 'epoch': 0.37}\n",
            "{'loss': 0.677, 'grad_norm': 0.20173096656799316, 'learning_rate': 0.00018, 'epoch': 0.37}\n",
            "{'loss': 0.4993, 'grad_norm': 0.16146717965602875, 'learning_rate': 0.000182, 'epoch': 0.38}\n",
            "{'loss': 0.6572, 'grad_norm': 0.19252893328666687, 'learning_rate': 0.00018400000000000003, 'epoch': 0.38}\n",
            "{'loss': 0.4621, 'grad_norm': 0.17994020879268646, 'learning_rate': 0.00018600000000000002, 'epoch': 0.39}\n",
            "{'loss': 0.4671, 'grad_norm': 0.17371797561645508, 'learning_rate': 0.000188, 'epoch': 0.39}\n",
            "{'loss': 0.5449, 'grad_norm': 0.2067899852991104, 'learning_rate': 0.00019, 'epoch': 0.39}\n",
            "{'loss': 0.4758, 'grad_norm': 0.1737201064825058, 'learning_rate': 0.000192, 'epoch': 0.4}\n",
            " 13% 96/723 [48:11<4:41:37, 26.95s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:13:24,493] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 0.6477174758911133, 'eval_runtime': 31.7126, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.315, 'epoch': 0.4}\n",
            " 13% 96/723 [48:43<4:41:37, 26.95s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.4704, 'grad_norm': 0.18918518722057343, 'learning_rate': 0.000194, 'epoch': 0.4}\n",
            "{'loss': 0.4098, 'grad_norm': 0.1849825531244278, 'learning_rate': 0.000196, 'epoch': 0.41}\n",
            "{'loss': 0.6009, 'grad_norm': 0.19550590217113495, 'learning_rate': 0.00019800000000000002, 'epoch': 0.41}\n",
            "{'loss': 0.6994, 'grad_norm': 0.22122766077518463, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
            "{'loss': 0.6341, 'grad_norm': 0.20585785806179047, 'learning_rate': 0.0001999987285691759, 'epoch': 0.42}\n",
            "{'loss': 0.6522, 'grad_norm': 0.19718682765960693, 'learning_rate': 0.0001999949143090342, 'epoch': 0.42}\n",
            "{'loss': 0.5892, 'grad_norm': 0.3182641565799713, 'learning_rate': 0.00019998855731656628, 'epoch': 0.43}\n",
            "{'loss': 0.45, 'grad_norm': 0.1560477763414383, 'learning_rate': 0.0001999796577534217, 'epoch': 0.43}\n",
            " 14% 104/723 [52:13<4:49:19, 28.04s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:17:26,517] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6443948745727539, 'eval_runtime': 31.7309, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 0.43}\n",
            " 14% 104/723 [52:45<4:49:19, 28.04s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5666, 'grad_norm': 0.18843381106853485, 'learning_rate': 0.00019996821584590405, 'epoch': 0.43}\n",
            "{'loss': 0.5009, 'grad_norm': 0.16280223429203033, 'learning_rate': 0.00019995423188496516, 'epoch': 0.44}\n",
            "{'loss': 0.5775, 'grad_norm': 0.23458942770957947, 'learning_rate': 0.00019993770622619782, 'epoch': 0.44}\n",
            "{'loss': 0.5057, 'grad_norm': 0.17502042651176453, 'learning_rate': 0.00019991863928982672, 'epoch': 0.45}\n",
            "{'loss': 0.6532, 'grad_norm': 0.18933424353599548, 'learning_rate': 0.00019989703156069762, 'epoch': 0.45}\n",
            "{'loss': 0.6224, 'grad_norm': 0.20085620880126953, 'learning_rate': 0.00019987288358826514, 'epoch': 0.46}\n",
            "{'loss': 0.4579, 'grad_norm': 0.16750240325927734, 'learning_rate': 0.00019984619598657893, 'epoch': 0.46}\n",
            "{'loss': 0.5983, 'grad_norm': 0.21059909462928772, 'learning_rate': 0.00019981696943426766, 'epoch': 0.46}\n",
            " 15% 112/723 [56:11<4:30:13, 26.54s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:21:24,335] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.6448244452476501, 'eval_runtime': 31.7002, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.315, 'epoch': 0.46}\n",
            " 15% 112/723 [56:43<4:30:13, 26.54s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.4218, 'grad_norm': 0.14533983170986176, 'learning_rate': 0.0001997852046745222, 'epoch': 0.47}\n",
            "{'loss': 0.4307, 'grad_norm': 0.17385542392730713, 'learning_rate': 0.00019975090251507638, 'epoch': 0.47}\n",
            "{'loss': 0.6154, 'grad_norm': 0.17939674854278564, 'learning_rate': 0.00019971406382818672, 'epoch': 0.48}\n",
            "{'loss': 0.5314, 'grad_norm': 0.1793249547481537, 'learning_rate': 0.00019967468955061, 'epoch': 0.48}\n",
            "{'loss': 0.6371, 'grad_norm': 0.17412781715393066, 'learning_rate': 0.0001996327806835797, 'epoch': 0.48}\n",
            "{'loss': 0.5111, 'grad_norm': 0.1912943422794342, 'learning_rate': 0.0001995883382927802, 'epoch': 0.49}\n",
            "{'loss': 0.404, 'grad_norm': 0.17106641829013824, 'learning_rate': 0.00019954136350832014, 'epoch': 0.49}\n",
            "{'loss': 0.378, 'grad_norm': 0.16122391819953918, 'learning_rate': 0.00019949185752470326, 'epoch': 0.5}\n",
            " 17% 120/723 [1:00:06<4:31:19, 27.00s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.34s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.03s/it]\u001b[A[2025-01-02 11:25:19,357] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6420243382453918, 'eval_runtime': 31.686, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.316, 'epoch': 0.5}\n",
            " 17% 120/723 [1:00:38<4:31:19, 27.00s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5665, 'grad_norm': 0.17536883056163788, 'learning_rate': 0.0001994398216007982, 'epoch': 0.5}\n",
            "{'loss': 0.6153, 'grad_norm': 0.178410142660141, 'learning_rate': 0.00019938525705980652, 'epoch': 0.51}\n",
            "{'loss': 0.4758, 'grad_norm': 0.17779971659183502, 'learning_rate': 0.00019932816528922905, 'epoch': 0.51}\n",
            "{'loss': 0.4746, 'grad_norm': 0.18094688653945923, 'learning_rate': 0.00019926854774083048, 'epoch': 0.51}\n",
            "{'loss': 0.365, 'grad_norm': 0.16100001335144043, 'learning_rate': 0.0001992064059306026, 'epoch': 0.52}\n",
            "{'loss': 0.4426, 'grad_norm': 0.16794076561927795, 'learning_rate': 0.0001991417414387257, 'epoch': 0.52}\n",
            "{'loss': 0.5563, 'grad_norm': 0.19279898703098297, 'learning_rate': 0.00019907455590952827, 'epoch': 0.53}\n",
            "{'loss': 0.695, 'grad_norm': 0.19863510131835938, 'learning_rate': 0.00019900485105144543, 'epoch': 0.53}\n",
            " 18% 128/723 [1:04:13<4:22:58, 26.52s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.34s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.03s/it]\u001b[A[2025-01-02 11:29:26,327] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6423586010932922, 'eval_runtime': 31.6476, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 0.53}\n",
            " 18% 128/723 [1:04:45<4:22:58, 26.52s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.3164, 'grad_norm': 0.1356392800807953, 'learning_rate': 0.0001989326286369753, 'epoch': 0.53}\n",
            "{'loss': 0.6856, 'grad_norm': 0.2080593705177307, 'learning_rate': 0.00019885789050263386, 'epoch': 0.54}\n",
            "{'loss': 0.4657, 'grad_norm': 0.1512424200773239, 'learning_rate': 0.00019878063854890856, 'epoch': 0.54}\n",
            "{'loss': 0.6154, 'grad_norm': 0.21698357164859772, 'learning_rate': 0.00019870087474020966, 'epoch': 0.55}\n",
            "{'loss': 0.4675, 'grad_norm': 0.16810199618339539, 'learning_rate': 0.00019861860110482048, 'epoch': 0.55}\n",
            "{'loss': 0.5508, 'grad_norm': 0.18953613936901093, 'learning_rate': 0.00019853381973484574, 'epoch': 0.55}\n",
            "{'loss': 0.5868, 'grad_norm': 0.17797330021858215, 'learning_rate': 0.00019844653278615833, 'epoch': 0.56}\n",
            "{'loss': 0.6498, 'grad_norm': 0.16277410089969635, 'learning_rate': 0.00019835674247834469, 'epoch': 0.56}\n",
            " 19% 136/723 [1:08:15<4:30:49, 27.68s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.97s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:33:27,984] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.639706015586853, 'eval_runtime': 31.6425, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 0.56}\n",
            " 19% 136/723 [1:08:47<4:30:49, 27.68s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.3912, 'grad_norm': 0.16014868021011353, 'learning_rate': 0.00019826445109464804, 'epoch': 0.57}\n",
            "{'loss': 0.5469, 'grad_norm': 0.16939055919647217, 'learning_rate': 0.0001981696609819106, 'epoch': 0.57}\n",
            "{'loss': 0.3456, 'grad_norm': 0.21310453116893768, 'learning_rate': 0.00019807237455051385, 'epoch': 0.58}\n",
            "{'loss': 0.6883, 'grad_norm': 0.1767939180135727, 'learning_rate': 0.00019797259427431705, 'epoch': 0.58}\n",
            "{'loss': 0.5966, 'grad_norm': 0.16658908128738403, 'learning_rate': 0.00019787032269059462, 'epoch': 0.58}\n",
            "{'loss': 0.5848, 'grad_norm': 0.1871192902326584, 'learning_rate': 0.00019776556239997146, 'epoch': 0.59}\n",
            "{'loss': 0.5802, 'grad_norm': 0.19814512133598328, 'learning_rate': 0.00019765831606635679, 'epoch': 0.59}\n",
            "{'loss': 0.5955, 'grad_norm': 0.18908849358558655, 'learning_rate': 0.0001975485864168765, 'epoch': 0.6}\n",
            " 20% 144/723 [1:12:20<4:24:04, 27.37s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:37:33,455] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6371442079544067, 'eval_runtime': 31.7157, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.315, 'epoch': 0.6}\n",
            " 20% 144/723 [1:12:52<4:24:04, 27.37s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.6326, 'grad_norm': 0.20401564240455627, 'learning_rate': 0.0001974363762418038, 'epoch': 0.6}\n",
            "{'loss': 0.4559, 'grad_norm': 0.15343907475471497, 'learning_rate': 0.00019732168839448815, 'epoch': 0.6}\n",
            "{'loss': 0.8286, 'grad_norm': 0.20439064502716064, 'learning_rate': 0.00019720452579128286, 'epoch': 0.61}\n",
            "{'loss': 0.3709, 'grad_norm': 0.18316370248794556, 'learning_rate': 0.00019708489141147083, 'epoch': 0.61}\n",
            "{'loss': 0.515, 'grad_norm': 0.15305231511592865, 'learning_rate': 0.00019696278829718883, 'epoch': 0.62}\n",
            "{'loss': 0.477, 'grad_norm': 0.1709292083978653, 'learning_rate': 0.00019683821955335012, 'epoch': 0.62}\n",
            "{'loss': 0.4123, 'grad_norm': 0.15829913318157196, 'learning_rate': 0.00019671118834756548, 'epoch': 0.63}\n",
            "{'loss': 0.3488, 'grad_norm': 0.14310665428638458, 'learning_rate': 0.00019658169791006276, 'epoch': 0.63}\n",
            " 21% 152/723 [1:16:15<4:09:20, 26.20s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:41:27,547] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6367970705032349, 'eval_runtime': 31.6852, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.316, 'epoch': 0.63}\n",
            " 21% 152/723 [1:16:46<4:09:20, 26.20s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.4887, 'grad_norm': 0.17535348236560822, 'learning_rate': 0.00019644975153360462, 'epoch': 0.63}\n",
            "{'loss': 0.4327, 'grad_norm': 0.15124952793121338, 'learning_rate': 0.0001963153525734049, 'epoch': 0.64}\n",
            "{'loss': 0.515, 'grad_norm': 0.1795273721218109, 'learning_rate': 0.00019617850444704314, 'epoch': 0.64}\n",
            "{'loss': 0.4556, 'grad_norm': 0.14841541647911072, 'learning_rate': 0.00019603921063437793, 'epoch': 0.65}\n",
            "{'loss': 0.6213, 'grad_norm': 0.17854200303554535, 'learning_rate': 0.00019589747467745817, 'epoch': 0.65}\n",
            "{'loss': 0.6797, 'grad_norm': 0.18727411329746246, 'learning_rate': 0.0001957533001804332, 'epoch': 0.65}\n",
            "{'loss': 0.5563, 'grad_norm': 0.20544157922267914, 'learning_rate': 0.00019560669080946093, 'epoch': 0.66}\n",
            "{'loss': 0.7381, 'grad_norm': 0.17448167502880096, 'learning_rate': 0.0001954576502926149, 'epoch': 0.66}\n",
            " 22% 160/723 [1:20:16<4:16:46, 27.36s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.36s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.03s/it]\u001b[A[2025-01-02 11:45:28,726] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6347053647041321, 'eval_runtime': 31.649, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 0.66}\n",
            " 22% 160/723 [1:20:47<4:16:46, 27.36s/it]\n",
            "100% 10/10 [00:29<00:00,  3.04s/it]\u001b[A\n",
            "{'loss': 0.6129, 'grad_norm': 0.18101713061332703, 'learning_rate': 0.00019530618241978922, 'epoch': 0.67}\n",
            "{'loss': 0.553, 'grad_norm': 0.16324813663959503, 'learning_rate': 0.00019515229104260235, 'epoch': 0.67}\n",
            "{'loss': 0.4506, 'grad_norm': 0.15541882812976837, 'learning_rate': 0.0001949959800742991, 'epoch': 0.67}\n",
            "{'loss': 0.3852, 'grad_norm': 0.14814035594463348, 'learning_rate': 0.00019483725348965112, 'epoch': 0.68}\n",
            "{'loss': 0.457, 'grad_norm': 0.1523849219083786, 'learning_rate': 0.00019467611532485588, 'epoch': 0.68}\n",
            "{'loss': 0.5331, 'grad_norm': 0.18017688393592834, 'learning_rate': 0.00019451256967743393, 'epoch': 0.69}\n",
            "{'loss': 0.4389, 'grad_norm': 0.15552937984466553, 'learning_rate': 0.00019434662070612487, 'epoch': 0.69}\n",
            "{'loss': 0.4642, 'grad_norm': 0.16257044672966003, 'learning_rate': 0.0001941782726307814, 'epoch': 0.7}\n",
            " 23% 168/723 [1:24:28<4:20:48, 28.20s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.99s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:49:41,404] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.633299708366394, 'eval_runtime': 31.7307, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 0.7}\n",
            " 23% 168/723 [1:25:00<4:20:48, 28.20s/it]\n",
            "100% 10/10 [00:29<00:00,  3.06s/it]\u001b[A\n",
            "{'loss': 0.3651, 'grad_norm': 0.14879409968852997, 'learning_rate': 0.0001940075297322622, 'epoch': 0.7}\n",
            "{'loss': 0.6335, 'grad_norm': 0.18363599479198456, 'learning_rate': 0.00019383439635232294, 'epoch': 0.7}\n",
            "{'loss': 0.5056, 'grad_norm': 0.186947301030159, 'learning_rate': 0.00019365887689350598, 'epoch': 0.71}\n",
            "{'loss': 0.5708, 'grad_norm': 0.17291715741157532, 'learning_rate': 0.00019348097581902823, 'epoch': 0.71}\n",
            "{'loss': 0.3839, 'grad_norm': 0.14972642064094543, 'learning_rate': 0.00019330069765266793, 'epoch': 0.72}\n",
            "{'loss': 0.3411, 'grad_norm': 0.15430015325546265, 'learning_rate': 0.00019311804697864948, 'epoch': 0.72}\n",
            "{'loss': 0.3547, 'grad_norm': 0.15433776378631592, 'learning_rate': 0.00019293302844152675, 'epoch': 0.72}\n",
            "{'loss': 0.4463, 'grad_norm': 0.170135036110878, 'learning_rate': 0.00019274564674606523, 'epoch': 0.73}\n",
            " 24% 176/723 [1:28:24<4:08:32, 27.26s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.34s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 11:53:37,412] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6315526962280273, 'eval_runtime': 31.6923, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.316, 'epoch': 0.73}\n",
            " 24% 176/723 [1:28:56<4:08:32, 27.26s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5044, 'grad_norm': 0.1866879016160965, 'learning_rate': 0.00019255590665712214, 'epoch': 0.73}\n",
            "{'loss': 0.5918, 'grad_norm': 0.17831812798976898, 'learning_rate': 0.00019236381299952542, 'epoch': 0.74}\n",
            "{'loss': 0.7641, 'grad_norm': 0.17055727541446686, 'learning_rate': 0.00019216937065795104, 'epoch': 0.74}\n",
            "{'loss': 0.6859, 'grad_norm': 0.18673548102378845, 'learning_rate': 0.00019197258457679878, 'epoch': 0.75}\n",
            "{'loss': 0.5536, 'grad_norm': 0.17935681343078613, 'learning_rate': 0.00019177345976006634, 'epoch': 0.75}\n",
            "{'loss': 0.621, 'grad_norm': 0.20911870896816254, 'learning_rate': 0.00019157200127122237, 'epoch': 0.75}\n",
            "{'loss': 0.4456, 'grad_norm': 0.1580791473388672, 'learning_rate': 0.0001913682142330775, 'epoch': 0.76}\n",
            "{'loss': 0.4896, 'grad_norm': 0.17067115008831024, 'learning_rate': 0.0001911621038276542, 'epoch': 0.76}\n",
            " 25% 184/723 [1:32:31<4:10:23, 27.87s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.34s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.85s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.03s/it]\u001b[A[2025-01-02 11:57:44,069] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6288789510726929, 'eval_runtime': 31.6241, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 0.76}\n",
            " 25% 184/723 [1:33:03<4:10:23, 27.87s/it]\n",
            "100% 10/10 [00:29<00:00,  3.04s/it]\u001b[A\n",
            "{'loss': 0.598, 'grad_norm': 0.18913179636001587, 'learning_rate': 0.0001909536752960549, 'epoch': 0.77}\n",
            "{'loss': 0.4867, 'grad_norm': 0.17750480771064758, 'learning_rate': 0.00019074293393832876, 'epoch': 0.77}\n",
            "{'loss': 0.7741, 'grad_norm': 0.19708502292633057, 'learning_rate': 0.000190529885113337, 'epoch': 0.77}\n",
            "{'loss': 0.5342, 'grad_norm': 0.20043936371803284, 'learning_rate': 0.00019031453423861645, 'epoch': 0.78}\n",
            "{'loss': 0.565, 'grad_norm': 0.173015758395195, 'learning_rate': 0.0001900968867902419, 'epoch': 0.78}\n",
            "{'loss': 0.3887, 'grad_norm': 0.15339209139347076, 'learning_rate': 0.0001898769483026869, 'epoch': 0.79}\n",
            "{'loss': 0.4818, 'grad_norm': 0.15938282012939453, 'learning_rate': 0.00018965472436868286, 'epoch': 0.79}\n",
            "{'loss': 0.4838, 'grad_norm': 0.16572917997837067, 'learning_rate': 0.00018943022063907695, 'epoch': 0.8}\n",
            " 27% 192/723 [1:36:26<3:48:52, 25.86s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.36s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.87s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 12:01:39,134] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6294319033622742, 'eval_runtime': 31.7242, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 0.8}\n",
            " 27% 192/723 [1:36:58<3:48:52, 25.86s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.3637, 'grad_norm': 0.15262435376644135, 'learning_rate': 0.00018920344282268847, 'epoch': 0.8}\n",
            "{'loss': 0.5963, 'grad_norm': 0.17313385009765625, 'learning_rate': 0.0001889743966861635, 'epoch': 0.8}\n",
            "{'loss': 0.4494, 'grad_norm': 0.16489067673683167, 'learning_rate': 0.0001887430880538284, 'epoch': 0.81}\n",
            "{'loss': 0.6596, 'grad_norm': 0.19257767498493195, 'learning_rate': 0.00018850952280754165, 'epoch': 0.81}\n",
            "{'loss': 0.5384, 'grad_norm': 0.15497300028800964, 'learning_rate': 0.00018827370688654437, 'epoch': 0.82}\n",
            "{'loss': 0.6597, 'grad_norm': 0.16723990440368652, 'learning_rate': 0.00018803564628730915, 'epoch': 0.82}\n",
            "{'loss': 0.4727, 'grad_norm': 0.17013424634933472, 'learning_rate': 0.00018779534706338767, 'epoch': 0.82}\n",
            "{'loss': 0.6406, 'grad_norm': 0.16207966208457947, 'learning_rate': 0.00018755281532525673, 'epoch': 0.83}\n",
            " 28% 200/723 [1:40:26<3:53:46, 26.82s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.34s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.01s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.03s/it]\u001b[A[2025-01-02 12:05:39,044] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6287280321121216, 'eval_runtime': 31.6246, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.316, 'epoch': 0.83}\n",
            " 28% 200/723 [1:40:58<3:53:46, 26.82s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5257, 'grad_norm': 0.176325261592865, 'learning_rate': 0.0001873080572401629, 'epoch': 0.83}\n",
            "{'loss': 0.6827, 'grad_norm': 0.18010878562927246, 'learning_rate': 0.00018706107903196565, 'epoch': 0.84}\n",
            "{'loss': 0.6653, 'grad_norm': 0.19952958822250366, 'learning_rate': 0.00018681188698097914, 'epoch': 0.84}\n",
            "{'loss': 0.5516, 'grad_norm': 0.18559566140174866, 'learning_rate': 0.00018656048742381241, 'epoch': 0.84}\n",
            "{'loss': 0.384, 'grad_norm': 0.17348656058311462, 'learning_rate': 0.00018630688675320842, 'epoch': 0.85}\n",
            "{'loss': 0.5614, 'grad_norm': 0.20258967578411102, 'learning_rate': 0.00018605109141788136, 'epoch': 0.85}\n",
            "{'loss': 0.6386, 'grad_norm': 0.2170458287000656, 'learning_rate': 0.0001857931079223527, 'epoch': 0.86}\n",
            "{'loss': 0.4792, 'grad_norm': 0.15862201154232025, 'learning_rate': 0.00018553294282678583, 'epoch': 0.86}\n",
            " 29% 208/723 [1:44:21<3:40:11, 25.65s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.63s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.87s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.94s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.99s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 12:09:33,797] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6266509890556335, 'eval_runtime': 31.7419, 'eval_samples_per_second': 0.63, 'eval_steps_per_second': 0.315, 'epoch': 0.86}\n",
            " 29% 208/723 [1:44:52<3:40:11, 25.65s/it]\n",
            "100% 10/10 [00:29<00:00,  3.06s/it]\u001b[A\n",
            "{'loss': 0.6017, 'grad_norm': 0.18450690805912018, 'learning_rate': 0.00018527060274681914, 'epoch': 0.87}\n",
            "{'loss': 0.41, 'grad_norm': 0.19569207727909088, 'learning_rate': 0.00018500609435339796, 'epoch': 0.87}\n",
            "{'loss': 0.3629, 'grad_norm': 0.15888173878192902, 'learning_rate': 0.00018473942437260475, 'epoch': 0.87}\n",
            "{'loss': 0.4038, 'grad_norm': 0.1897428184747696, 'learning_rate': 0.0001844705995854882, 'epoch': 0.88}\n",
            "{'loss': 0.589, 'grad_norm': 0.20784138143062592, 'learning_rate': 0.00018419962682789074, 'epoch': 0.88}\n",
            "{'loss': 0.4235, 'grad_norm': 0.15685002505779266, 'learning_rate': 0.00018392651299027466, 'epoch': 0.89}\n",
            "{'loss': 0.5648, 'grad_norm': 0.18681152164936066, 'learning_rate': 0.000183651265017547, 'epoch': 0.89}\n",
            "{'loss': 0.5987, 'grad_norm': 0.18050312995910645, 'learning_rate': 0.0001833738899088829, 'epoch': 0.89}\n",
            " 30% 216/723 [1:48:14<3:43:50, 26.49s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.87s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.94s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.99s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 12:13:26,989] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6269541382789612, 'eval_runtime': 31.7753, 'eval_samples_per_second': 0.629, 'eval_steps_per_second': 0.315, 'epoch': 0.89}\n",
            " 30% 216/723 [1:48:46<3:43:50, 26.49s/it]\n",
            "100% 10/10 [00:29<00:00,  3.06s/it]\u001b[A\n",
            "{'loss': 0.4814, 'grad_norm': 0.22043375670909882, 'learning_rate': 0.00018309439471754765, 'epoch': 0.9}\n",
            "{'loss': 0.5755, 'grad_norm': 0.15307611227035522, 'learning_rate': 0.00018281278655071724, 'epoch': 0.9}\n",
            "{'loss': 0.5069, 'grad_norm': 0.1558406502008438, 'learning_rate': 0.00018252907256929775, 'epoch': 0.91}\n",
            "{'loss': 0.5353, 'grad_norm': 0.18514493107795715, 'learning_rate': 0.00018224325998774323, 'epoch': 0.91}\n",
            "{'loss': 0.6523, 'grad_norm': 0.20324932038784027, 'learning_rate': 0.00018195535607387219, 'epoch': 0.92}\n",
            "{'loss': 0.6401, 'grad_norm': 0.19234046339988708, 'learning_rate': 0.0001816653681486828, 'epoch': 0.92}\n",
            "{'loss': 0.615, 'grad_norm': 0.19284357130527496, 'learning_rate': 0.00018137330358616684, 'epoch': 0.92}\n",
            "{'loss': 0.596, 'grad_norm': 0.1925787478685379, 'learning_rate': 0.00018107916981312204, 'epoch': 0.93}\n",
            " 31% 224/723 [1:52:04<3:34:27, 25.79s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.35s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.69s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.86s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.93s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.99s/it]\u001b[A\n",
            " 80% 8/10 [00:22<00:06,  3.02s/it]\u001b[A\n",
            " 90% 9/10 [00:25<00:03,  3.04s/it]\u001b[A[2025-01-02 12:17:16,577] [INFO] [accelerate.accelerator.gather_for_metrics:2509] [PID:6219] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
            "\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 0.6265174150466919, 'eval_runtime': 31.7097, 'eval_samples_per_second': 0.631, 'eval_steps_per_second': 0.315, 'epoch': 0.93}\n",
            " 31% 224/723 [1:52:35<3:34:27, 25.79s/it]\n",
            "100% 10/10 [00:29<00:00,  3.05s/it]\u001b[A\n",
            "{'loss': 0.5272, 'grad_norm': 0.16309843957424164, 'learning_rate': 0.00018078297430896333, 'epoch': 0.93}\n",
            "{'loss': 0.4047, 'grad_norm': 0.17369648814201355, 'learning_rate': 0.00018048472460553257, 'epoch': 0.94}\n",
            "{'loss': 0.4646, 'grad_norm': 0.16463559865951538, 'learning_rate': 0.0001801844282869071, 'epoch': 0.94}\n",
            "{'loss': 0.4502, 'grad_norm': 0.1607687920331955, 'learning_rate': 0.00017988209298920685, 'epoch': 0.94}\n",
            "{'loss': 0.6284, 'grad_norm': 0.1630995273590088, 'learning_rate': 0.00017957772640040008, 'epoch': 0.95}\n",
            "{'loss': 0.4157, 'grad_norm': 0.14142777025699615, 'learning_rate': 0.00017927133626010814, 'epoch': 0.95}\n",
            "{'loss': 0.376, 'grad_norm': 0.1450684815645218, 'learning_rate': 0.0001789629303594083, 'epoch': 0.96}\n",
            "{'loss': 0.6075, 'grad_norm': 0.18701522052288055, 'learning_rate': 0.00017865251654063604, 'epoch': 0.96}\n",
            " 32% 232/723 [1:56:21<3:52:26, 28.40s/it]\n",
            "  0% 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 20% 2/10 [00:03<00:13,  1.64s/it]\u001b[A\n",
            " 30% 3/10 [00:06<00:16,  2.36s/it]\u001b[A\n",
            " 40% 4/10 [00:09<00:16,  2.70s/it]\u001b[A\n",
            " 50% 5/10 [00:13<00:14,  2.87s/it]\u001b[A\n",
            " 60% 6/10 [00:16<00:11,  2.94s/it]\u001b[A\n",
            " 70% 7/10 [00:19<00:08,  2.98s/it]\u001b[A"
          ]
        }
      ],
      "source": [
        "!accelerate launch -m axolotl.cli.train /content/tinyllama.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpQTBSvdAG9R"
      },
      "source": [
        "Predict with trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFPYRGQmAG9R"
      },
      "outputs": [],
      "source": [
        "!accelerate launch -m axolotl.cli.inference /content/tinyllama.yaml \\\n",
        "    --lora_model_dir=\"./outputs/lora-out\" --gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMUk10H_AG9R"
      },
      "source": [
        "## Deeper Dive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUAybYKAG9R"
      },
      "source": [
        "It is also helpful to gain some familiarity over some of the core inner workings of axolotl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4FnoqzwAG9R"
      },
      "source": [
        "## Configuration Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gz80EOoAG9S"
      },
      "source": [
        "Axolotl uses a custom Dict class, called ```DictDefault```\n",
        "to store configurations specified in the yaml configuration file (into a Python variable named ```cfg```). The definition for this custom Dict can be found in the [utils/dict.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/utils/dict.py)\n",
        "\n",
        "```DictDefault``` is amended such that calling a missing key from it will result in a ```None``` return type. This is important because if some configuration options aren't specified by the user, the ```None``` type allows Axolotl to perform boolean operations to determine the default settings for missing configurations. For more examples on how this is done, check out [utils/config/__init__.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/utils/config/__init__.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKD29qTPAG9S"
      },
      "source": [
        "## Loading Models, Tokenizers, and Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru3KcGH3AG9S"
      },
      "source": [
        "If we inspect [cli.train.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/cli/train.py), we will find that most of the heavy lifting were done by the function ```train()``` which is itself imported from [src/axolotl/train.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/train.py).\n",
        "\n",
        "```train()``` takes care of loading the appropriate tokenizer and pre-trained model through ```load_model()``` and ```load_tokenizer()``` from [src/axolotl/utils/models.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/utils/models.py) respectively.\n",
        "\n",
        "```load_tokenizer()``` loads in the appropriate tokenizer given the desired model, as well as chat templates.\n",
        "\n",
        "```ModelLoader``` class follows after tokenizer has been selected. It will automatically discern the base model type, load in the desired model, as well as applying model-appropriate attention mechanism modifications (e.g. flash attention). Depending on which base model the user chooses in the configuration, ```ModelLoader``` will utilize the corresponding \"attention hijacking\" script. For example, if the user specified the base model to be ```NousResearch/Meta-Llama-3.1-8B```, which is of llama type, and set ```flash_attn``` to ```True```, ```ModelLoader``` will load in [llama_attn_hijack_flash.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/monkeypatch/llama_attn_hijack_flash.py). For a list of supported attention hijacking, please refer to the directory [/src/axolotl/monkeypatch/](https://github.com/axolotl-ai-cloud/axolotl/tree/main/src/axolotl/monkeypatch)\n",
        "\n",
        "Another important operation encompassed in ```train()``` is setting up the training that takes into account of user-specified traning configurations (e.g. num_epochs, optimizer) through the use of ```setup_trainer()``` from [/src/axolotl/utils/trainer.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/utils/trainer.py), which in turn relies on modules from [/src/axolotl/core/trainer_builder.py](https://github.com/axolotl-ai-cloud/axolotl/blob/main/src/axolotl/core/trainer_builder.py).\n",
        "```trainer_builder.py``` provides a list of trainer object options bespoke for the task type (Causal or Reinforcement learning ('dpo', 'ipo', 'kto') )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3HuTOC4AG9S"
      },
      "source": [
        "## Monkey patch\n",
        "\n",
        "The [Monkey patch directory](https://github.com/axolotl-ai-cloud/axolotl/tree/main/src/axolotl/monkeypatch) is where model architecture/optimization patching scripts are stored (these are modifications that are not implemented in the official releases, hence the name monkey patch). It includes attention jacking, ReLoRA, and unsloth optimization."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b70153ca8d034b31bbdd009707aa97c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_f0084a9344c64327922190e1f3041d96"
          }
        },
        "f0084a9344c64327922190e1f3041d96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}